data:
  dataset:
    name: coco
    dataset_root: /work/vjsalt22/dataset/coco/
  batch_size: 64
  dev_batch_size: 2
  split_ratio: 0.9

log_setting:
  log_detokenize_results: true # whether or not to save the results of detokenized vq output
  log_detokenize_results_every_n_epoch: 5
  log_draw_pca_every_n_epoch: 0

vq:
  bn_before_vq : true
  activation: gelu # relu or gelu
  type: SimpleVectorQuantizer # which type of quantizer to use, gumbel or kmeans
  args:
    temp: fixed=0.1 #[2.0, 0.5, 0.999995]
    time_first: true
    use_gumbel: false
    hard: true

cl_loss:
  type: MaskedContrastiveLoss # SupConLoss
  args:
    temperature: 0.07
    temperature_trainable: False
    margin: 0.0
    dcl: false
    a2b:  true
    b2a: true

    # for SupConLoss
    # temperature: 1.0
    # base_temperature: 1.0
    # contrast_mode: all
    # learnable_temperature: true
retrieval:
  recall_at: [1,5,10]

clip:
  name: ViT-B/32
  image_encoder_trainable: false
  text_encoder_trainable: false
  reduce_subword_embbedding: ./avssl/data/coco_stat/text_clip_vocab_usage_byfreq.npy # npy file for the selected embedding ID and its frequency

keyword:
  attention_heads: 1
  number: 8
  detokenized_K_neighbors: 10
  retrieve_method: cosine # cosine or pseudo_inverse
  batchnorms:
    type: eachKw #eachKw or same
    std_scale: 1.0
    learnable: true
    parallel: true

audio_encoder:
  type: s3prl
  name: hubert
  pretrained: true
  trainable: false
  feat_select_idx: all #last_hidden_state # all
  layer_drop: 0.0
  max_audio_len: 95573 #102400
  # reinit_layers: [10, 11]
  optim:
    name: Adam
    args:
      lr: 1.e-4
      weight_decay: 1.e-6
  scheduler:
    name: linear_warmup_decay
    warmup: 40000
    max_step: 4000000
    final_lr: 1.e-8

trainer:
  max_steps: 400000
  gradient_clip_val: 4
  accumulate_grad_batches: 1
  check_val_every_n_epoch: 1
  precision: 16
  logger: true
  log_every_n_steps: 8
  default_root_dir: exp/COCO/1e-4
  num_sanity_val_steps: 0
  strategy: ddp
  # accelerator: dp
  # limit_train_batches: 1
  #limit_val_batches: 8

data:
  dataset:
    name: flickr
    dataset_root: /home/twsezjg982/dataset/flickr/
    text_file: Flickr8k.token.txt
  batch_size: 12
  dev_batch_size: 8
  split_ratio: 0.9

log_setting:
  log_detokenize_results: false # whether or not to save the results of detokenized vq output

cl_loss:
  temperature: 1
  base_temperature: 1
  contrast_mode: all

retrieval:
  recall_at: [1,5,10]

clip:
  name: ViT-B/32
  image_encoder_trainable: false
  text_encoder_trainable: false
  # reduce_subword_embbedding: ./avssl/data/flickr_stat/text_clip_vocab_usage_byfreq.npy # npy file for the selected embedding ID and its frequency

audio_encoder:
  type: s3prl
  name: hubert
  pretrained: true
  trainable: true
  feat_select_idx: last_hidden_state
  layer_drop: 0.05
  max_audio_len: 102400
  optim:
    name: Adam
    args:
      lr: 1.e-4
      weight_decay: 1.e-6
  scheduler:
    name: linear_warmup_decay
    warmup: 5000
    max_step: 50000
    final_lr: 1.e-8

trainer:
  max_steps: 50000
  gradient_clip_val: 5
  accumulate_grad_batches: 8
  check_val_every_n_epoch: 1
  precision: 16
  logger: true
  log_every_n_steps: 1
  default_root_dir: exp/sc_c/kw
  num_sanity_val_steps: 0
  # accelerator: dp
  # limit_train_batches: 1
  # limit_val_batches: 16
